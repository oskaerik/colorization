{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from skimage import transform\n",
    "from torch.utils import data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from colorize.util import reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_model_metrics(model, val_loader, val_name):\n",
    "    \"\"\"Get the model metrics (non-balanced AUC, balanced AUC, accuracy) for a comparison model on validation dataset\"\"\"\n",
    "    Y_pred = []\n",
    "    y_true = []\n",
    "    \n",
    "    for X, y in tqdm(val_loader):\n",
    "        Y_pred.append(model(X).softmax(dim=-1).detach().numpy())\n",
    "        y_true.append(y.numpy())\n",
    "        \n",
    "    Y_pred = np.vstack(Y_pred)\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred_labels = np.argmax(Y_pred, axis=1)\n",
    "    \n",
    "    np.save(f'data/{val_name}_Y_pred', Y_pred)\n",
    "    np.save(f'data/{val_name}_y_true', y_true)\n",
    "    np.save(f'data/{val_name}_y_pred_labels', y_pred_labels)\n",
    "    \n",
    "    AUC_macro = roc_auc_score(y_true, Y_pred, multi_class=\"ovo\", average=\"macro\")\n",
    "    AUC_weighted = roc_auc_score(y_true, Y_pred, multi_class=\"ovo\", average=\"weighted\")\n",
    "    acc = accuracy_score(y_true, y_pred_labels)\n",
    "    \n",
    "    return {'AUC macro': AUC_macro, 'AUC weighted': AUC_weighted, 'Accuracy': acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Inception V3 model pretrained on Imagenet \n",
    "inceptionV3 = models.inception_v3(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VGG16 Batch Norm model pretrained on Imagenet\n",
    "VGG16 = models.vgg16_bn(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder for Imagenet validation set \n",
    "val_folder = 'data/ILSVRC2012_img_val/'\n",
    "\n",
    "# Dataloader constants \n",
    "BS = 40\n",
    "N_WORKERS = 1\n",
    "\n",
    "# Normalizer for Imagenet\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        datasets.ImageFolder(val_folder, transforms.Compose([\n",
    "            transforms.Scale(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])),\n",
    "        batch_size=BS, shuffle=False,\n",
    "        num_workers=N_WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = comparison_model_metrics(VGG16, val_loader, 'imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
