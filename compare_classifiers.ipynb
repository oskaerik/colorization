{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from skimage import transform\n",
    "from torch.utils import data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from colorize.util import reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_model_metrics(model, model_name, val_loader, val_name, index_mapper=None):\n",
    "    \"\"\"Get the model metrics (non-balanced AUC, balanced AUC, accuracy) for a comparison model on validation dataset\"\"\"\n",
    "    Y_pred = []\n",
    "    y_true = []\n",
    "    \n",
    "    for X, y in tqdm(val_loader):\n",
    "        output = model(X)\n",
    "        if index_mapper is not None:\n",
    "            output = output[index_mapper]\n",
    "        Y_pred.append(output.softmax(dim=-1).detach().numpy())\n",
    "        y_true.append(y.numpy())\n",
    "        \n",
    "    Y_pred = np.vstack(Y_pred)\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred_labels = np.argmax(Y_pred, axis=1)\n",
    "    \n",
    "    np.save(f'data/{model_name}_{val_name}_Y_pred', Y_pred)\n",
    "    np.save(f'data/{model_name}_{val_name}_y_true', y_true)\n",
    "    np.save(f'data/{model_name}_{val_name}_y_pred_labels', y_pred_labels)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred_labels)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Inception V3 model pretrained on Imagenet \n",
    "inceptionV3 = models.inception_v3(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VGG16 Batch Norm model pretrained on Imagenet\n",
    "VGG16 = models.vgg16_bn(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder for Imagenet validation set \n",
    "imagenet_val_folder = 'data/ILSVRC2012_img_val/'\n",
    "\n",
    "# Dataloader constants \n",
    "BS = 40\n",
    "N_WORKERS = 1\n",
    "\n",
    "# Normalizer for Imagenet\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        datasets.ImageFolder(imagenet_val_folder, transforms.Compose([\n",
    "            transforms.Scale(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])),\n",
    "        batch_size=BS, shuffle=False,\n",
    "        num_workers=N_WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder for dogs dataset \n",
    "dogs_val_folder = 'data/test/'\n",
    "\n",
    "# Dataloader constants \n",
    "BS = 2\n",
    "N_WORKERS = 1\n",
    "\n",
    "# Normalizer for Imagenet\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "dogs_val_loader = torch.utils.data.DataLoader(\n",
    "        datasets.ImageFolder(dogs_val_folder, transforms.Compose([\n",
    "            transforms.Scale(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])),\n",
    "        batch_size=BS, shuffle=False,\n",
    "        num_workers=N_WORKERS, pin_memory=True)\n",
    "\n",
    "# Class to index mapping for dogs dataloader\n",
    "dogs_class_to_idx = dogs_val_loader.dataset.class_to_idx\n",
    "dogs_idx_to_class = {v: k.split('-')[0] for k, v in dogs_class_to_idx.items()}\n",
    "\n",
    "# Class to index mapping for imagenet dataloader\n",
    "imagenet_class_to_idx = imagenet_val_loader.dataset.class_to_idx\n",
    "\n",
    "# Mapper array to slice imagenet output and align with dogs dataset label \n",
    "imagenet_to_dogs = np.zeros(len(dogs_class_to_idx), dtype=int)\n",
    "for i in range(len(imagenet_to_dogs)):\n",
    "    imagenet_to_dogs[i] = imagenet_class_to_idx[dogs_idx_to_class[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics_imagenet = comparison_model_metrics(VGG16, 'VGG16BN', val_loader_imagenet, 'imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics_dogs = comparison_model_metrics(VGG16, 'VGG16BN', val_loader_dogs, 'dogs', imagenet_to_dogs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
